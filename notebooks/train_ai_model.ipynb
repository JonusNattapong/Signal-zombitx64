{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZombitX64 Trading Signal Model Training\n",
    "\n",
    "This notebook is used to train AI models for the trading signal system.\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, Bidirectional\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Add the parent directory to the path for imports\n",
    "sys.path.append('..')\n",
    "from app.core.market_data.fetcher import MarketDataFetcher\n",
    "from app.core.ai.indicators import calculate_all_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install safetensors if not already installed\n",
    "try:\n",
    "    import safetensors\n",
    "    print(f\"safetensors version: {safetensors.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing safetensors...\")\n",
    "    !pip install safetensors\n",
    "    import safetensors\n",
    "    print(f\"safetensors version: {safetensors.__version__}\")\n",
    "\n",
    "from safetensors.tensorflow import save_file, load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "Fetch historical market data for training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_historical_data(symbols, timeframes, limit=1000):\n",
    "    data_dict = {}\n",
    "    fetcher = MarketDataFetcher()\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        data_dict[symbol] = {}\n",
    "        for timeframe in timeframes:\n",
    "            print(f\"Fetching {symbol} {timeframe}...\")\n",
    "            df = await fetcher.fetch_data(symbol, timeframe, limit=limit)\n",
    "            if not df.empty:\n",
    "                # Calculate indicators\n",
    "                df = calculate_all_indicators(df)\n",
    "                data_dict[symbol][timeframe] = df\n",
    "                print(f\"  - Got {len(df)} rows with {len(df.columns)} features\")\n",
    "            else:\n",
    "                print(f\"  - No data available\")\n",
    "    \n",
    "    await fetcher.close()\n",
    "    return data_dict\n",
    "\n",
    "# Define symbols and timeframes\n",
    "symbols = ['BTCUSDT', 'ETHUSDT', 'EURUSD', 'GBPUSD']\n",
    "timeframes = ['1h', '4h', '1d']\n",
    "\n",
    "# Create event loop and run data collection\n",
    "import asyncio\n",
    "historical_data = asyncio.run(fetch_historical_data(symbols, timeframes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Feature Engineering\n",
    "\n",
    "Prepare the data for training by creating labels and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, window_size=60, future_bars=10):\n",
    "    \"\"\"\n",
    "    Prepare data for supervised learning:\n",
    "    - Create labels using future price movements\n",
    "    - Prepare feature windows\n",
    "    \n",
    "    Labels:\n",
    "    - 0: BUY (price goes up significantly)\n",
    "    - 1: SELL (price goes down significantly)\n",
    "    - 2: HOLD (price doesn't move significantly)\n",
    "    \"\"\"\n",
    "    # Clean data\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Create labels\n",
    "    future_returns = df['close'].pct_change(future_bars).shift(-future_bars)\n",
    "    threshold = df['atr'].rolling(window=20).mean() * 1.5 / df['close']\n",
    "    \n",
    "    # Default to HOLD\n",
    "    labels = np.full(len(df), 2)  # 2 = HOLD\n",
    "    \n",
    "    # Set BUY and SELL labels\n",
    "    labels[future_returns > threshold] = 0  # BUY\n",
    "    labels[future_returns < -threshold] = 1  # SELL\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        'open', 'high', 'low', 'close', 'rsi', 'macd', 'macd_signal', 'macd_hist',\n",
    "        'bb_upper', 'bb_middle', 'bb_lower', 'stoch_k', 'stoch_d', 'adx',\n",
    "        'ichi_tenkan', 'ichi_kijun', 'atr'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available features\n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    features_df = df[feature_cols]\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_df)\n",
    "    \n",
    "    # Create feature windows and matching labels\n",
    "    X, y = [], []\n",
    "    for i in range(len(features_scaled) - window_size - future_bars):\n",
    "        X.append(features_scaled[i:i+window_size])\n",
    "        y.append(labels[i+window_size])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler, feature_cols\n",
    "\n",
    "# Prepare data for one symbol/timeframe\n",
    "symbol = 'BTCUSDT'\n",
    "timeframe = '1h'\n",
    "X, y, scaler, feature_cols = prepare_data(historical_data[symbol][timeframe])\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # Don't shuffle for time series data\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(f\"Features used: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, output_classes=3):\n",
    "    model = Sequential([\n",
    "        LSTM(100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(output_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train LSTM model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (window_size, num_features)\n",
    "lstm_model = build_lstm_model(input_shape)\n",
    "\n",
    "# Training callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"LSTM Test Accuracy: {lstm_accuracy:.4f}\")\n",
    "\n",
    "# Save the model in Keras h5 format\n",
    "save_dir = \"../app/core/ai/models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "lstm_model.save(f\"{save_dir}/latest_lstm_model.h5\")\n",
    "\n",
    "# Save model weights in safetensors format\n",
    "# Convert weights to a tensor dictionary suitable for safetensors\n",
    "weight_dict = {}\n",
    "for i, layer in enumerate(lstm_model.layers):\n",
    "    layer_weights = layer.get_weights()\n",
    "    for j, weight in enumerate(layer_weights):\n",
    "        # Convert each weight to a tensor with a unique name\n",
    "        weight_dict[f\"layer_{i}_weight_{j}\"] = tf.convert_to_tensor(weight)\n",
    "\n",
    "# Save weights in safetensors format\n",
    "save_file(weight_dict, f\"{save_dir}/latest_lstm_model.safetensors\")\n",
    "print(f\"LSTM model weights saved in safetensors format\")\n",
    "\n",
    "# Also save scaler and feature columns\n",
    "model_metadata = {\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"input_shape\": input_shape,\n",
    "    \"accuracy\": float(lstm_accuracy)\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/latest_lstm_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GRU Model Training\n",
    "def build_gru_model(input_shape, output_classes=3):\n",
    "    model = Sequential([\n",
    "        GRU(100, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        GRU(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(output_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train GRU model\n",
    "gru_model = build_gru_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "gru_loss, gru_accuracy = gru_model.evaluate(X_test, y_test)\n",
    "print(f\"GRU Test Accuracy: {gru_accuracy:.4f}\")\n",
    "\n",
    "# Save the model in Keras h5 format\n",
    "gru_model.save(f\"{save_dir}/latest_gru_model.h5\")\n",
    "\n",
    "# Save model weights in safetensors format\n",
    "weight_dict = {}\n",
    "for i, layer in enumerate(gru_model.layers):\n",
    "    layer_weights = layer.get_weights()\n",
    "    for j, weight in enumerate(layer_weights):\n",
    "        weight_dict[f\"gru_layer_{i}_weight_{j}\"] = tf.convert_to_tensor(weight)\n",
    "\n",
    "# Save weights in safetensors format\n",
    "save_file(weight_dict, f\"{save_dir}/latest_gru_model.safetensors\")\n",
    "print(f\"GRU model weights saved in safetensors format\")\n",
    "\n",
    "# Also save GRU metadata\n",
    "gru_metadata = {\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"model_type\": \"GRU\",\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"input_shape\": input_shape,\n",
    "    \"accuracy\": float(gru_accuracy)\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/latest_gru_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(gru_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Random Forest Model\n",
    "\n",
    "# Reshape data for traditional ML models (flatten time windows)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Flattened feature shape: {X_train_flat.shape}\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Evaluate RF model\n",
    "rf_pred = rf_model.predict(X_test_flat)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(f\"Random Forest Test Accuracy: {rf_accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification metrics\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred, target_names=[\"Buy\", \"Sell\", \"Hold\"]))\n",
    "\n",
    "# Save the model\n",
    "rf_path = f\"{save_dir}/latest_random_forest_model.joblib\"\n",
    "joblib.dump(rf_model, rf_path)\n",
    "\n",
    "# Save scaler with the model\n",
    "rf_metadata = {\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"model_type\": \"RandomForest\",\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"accuracy\": float(rf_accuracy),\n",
    "    \"feature_importances\": rf_model.feature_importances_.tolist() if hasattr(rf_model, 'feature_importances_') else None\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/latest_random_forest_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(rf_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Gradient Boosting Model\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Evaluate GB model\n",
    "gb_pred = gb_model.predict(X_test_flat)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print(f\"Gradient Boosting Test Accuracy: {gb_accuracy:.4f}\")\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, gb_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Buy\", \"Sell\", \"Hold\"],\n",
    "            yticklabels=[\"Buy\", \"Sell\", \"Hold\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification metrics\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, gb_pred, target_names=[\"Buy\", \"Sell\", \"Hold\"]))\n",
    "\n",
    "# Save the model\n",
    "gb_path = f\"{save_dir}/latest_gradient_boost_model.joblib\"\n",
    "joblib.dump(gb_model, gb_path)\n",
    "\n",
    "# Save scaler with the model\n",
    "gb_metadata = {\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"model_type\": \"GradientBoosting\",\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"accuracy\": float(gb_accuracy),\n",
    "    \"feature_importances\": gb_model.feature_importances_.tolist() if hasattr(gb_model, 'feature_importances_') else None\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/latest_gradient_boost_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(gb_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. CNN-LSTM Hybrid Model\n",
    "\n",
    "def build_cnn_lstm_model(input_shape, output_classes=3):\n",
    "    model = Sequential([\n",
    "        # CNN layers\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # LSTM layers\n",
    "        LSTM(100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layers\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(output_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train CNN-LSTM model\n",
    "cnn_lstm_model = build_cnn_lstm_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "cnn_lstm_loss, cnn_lstm_accuracy = cnn_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"CNN-LSTM Test Accuracy: {cnn_lstm_accuracy:.4f}\")\n",
    "\n",
    "# Save the model in Keras h5 format\n",
    "cnn_lstm_model.save(f\"{save_dir}/latest_cnn_lstm_model.h5\")\n",
    "\n",
    "# Save model weights in safetensors format\n",
    "weight_dict = {}\n",
    "for i, layer in enumerate(cnn_lstm_model.layers):\n",
    "    layer_weights = layer.get_weights()\n",
    "    for j, weight in enumerate(layer_weights):\n",
    "        weight_dict[f\"cnn_lstm_layer_{i}_weight_{j}\"] = tf.convert_to_tensor(weight)\n",
    "\n",
    "# Save weights in safetensors format\n",
    "save_file(weight_dict, f\"{save_dir}/latest_cnn_lstm_model.safetensors\")\n",
    "print(f\"CNN-LSTM model weights saved in safetensors format\")\n",
    "\n",
    "# Save metadata\n",
    "cnn_lstm_metadata = {\n",
    "    \"scaler\": scaler,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"model_type\": \"CNN_LSTM\",\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"input_shape\": input_shape,\n",
    "    \"accuracy\": float(cnn_lstm_accuracy)\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/latest_cnn_lstm_model_metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(cnn_lstm_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Model Comparison\n",
    "\n",
    "# Collect model performance metrics\n",
    "models = {\n",
    "    'LSTM': lstm_accuracy,\n",
    "    'GRU': gru_accuracy,\n",
    "    'Random Forest': rf_accuracy,\n",
    "    'Gradient Boosting': gb_accuracy,\n",
    "    'CNN-LSTM': cnn_lstm_accuracy\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models.keys(), models.values(), color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add accuracy values above bars\n",
    "for i, (model, acc) in enumerate(models.items()):\n",
    "    plt.text(i, acc + 0.02, f'{acc:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{save_dir}/../model_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "print(f\"All models saved to {save_dir}\")\n",
    "print(\"\\nBest performing model: \" + max(models.items(), key=lambda x: x[1])[0])\n",
    "\n",
    "# Update ensemble config to include all models\n",
    "ensemble_config = {\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"type\": \"lstm\",\n",
    "            \"path\": \"latest_lstm_model.h5\",\n",
    "            \"weight\": 1.2\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gru\",\n",
    "            \"path\": \"latest_gru_model.h5\",\n",
    "            \"weight\": 1.1\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"random_forest\",\n",
    "            \"path\": \"latest_random_forest_model.joblib\",\n",
    "            \"weight\": 0.9\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_boost\",\n",
    "            \"path\": \"latest_gradient_boost_model.joblib\",\n",
    "            \"weight\": 1.0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"cnn_lstm\",\n",
    "            \"path\": \"latest_cnn_lstm_model.h5\",\n",
    "            \"weight\": 1.3\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save final ensemble configuration\n",
    "ensemble_path = f\"{save_dir}/latest_ensemble_model.pkl\"\n",
    "with open(ensemble_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_config, f)\n",
    "\n",
    "print(f\"Final ensemble model saved to {ensemble_path}\")\n",
    "\n",
    "# Load a model from safetensors to verify it works\n",
    "print(\"\\nVerifying safetensors model loading...\")\n",
    "try:\n",
    "    # Load the LSTM model weights from safetensors\n",
    "    loaded_weights = load_file(f\"{save_dir}/latest_lstm_model.safetensors\")\n",
    "    print(f\"Successfully loaded model weights: {len(loaded_weights)} tensor groups\")\n",
    "    print(\"Safetensors validation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading safetensors model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create an Ensemble Model\n",
    "\n",
    "# Let's define a wrapper for our models to standardize the predict interface\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model, is_flat=False, scaler=None):\n",
    "        self.model = model\n",
    "        self.is_flat = is_flat  # True for RF and GB, False for LSTM and GRU\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Prepare X for the model type\n",
    "        if self.is_flat:\n",
    "            # Flatten for traditional ML models\n",
    "            X_prep = X.reshape(X.shape[0], -1)\n",
    "        else:\n",
    "            # Keep dimensions for sequence models\n",
    "            X_prep = X\n",
    "        \n",
    "        # Get predictions\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probs = self.model.predict_proba(X_prep)\n",
    "            predictions = []\n",
    "            for prob in probs:\n",
    "                cls = np.argmax(prob)\n",
    "                confidence = prob[cls] * 100\n",
    "                predictions.append((cls, confidence))\n",
    "        else:\n",
    "            # For models without predict_proba\n",
    "            preds = self.model.predict(X_prep)\n",
    "            predictions = [(int(p), 80.0) for p in preds]  # Default 80% confidence\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create model wrappers\n",
    "lstm_wrapper = ModelWrapper(lstm_model, is_flat=False, scaler=scaler)\n",
    "gru_wrapper = ModelWrapper(gru_model, is_flat=False, scaler=scaler)\n",
    "rf_wrapper = ModelWrapper(rf_model, is_flat=True, scaler=scaler)\n",
    "gb_wrapper = ModelWrapper(gb_model, is_flat=True, scaler=scaler)\n",
    "\n",
    "# Create ensemble configuration\n",
    "ensemble_config = {\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"type\": \"lstm\",\n",
    "            \"path\": \"latest_lstm_model.h5\",\n",
    "            \"weight\": 1.2\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gru\",\n",
    "            \"path\": \"latest_gru_model.h5\",\n",
    "            \"weight\": 1.1\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"random_forest\",\n",
    "            \"path\": \"latest_random_forest_model.joblib\",\n",
    "            \"weight\": 0.9\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gradient_boost\",\n",
    "            \"path\": \"latest_gradient_boost_model.joblib\",\n",
    "            \"weight\": 1.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save ensemble configuration\n",
    "ensemble_path = f\"{save_dir}/latest_ensemble_model.pkl\"\n",
    "with open(ensemble_path, 'wb') as f:\n",
    "    pickle.dump(ensemble_config, f)\n",
    "\n",
    "print(f\"Ensemble model configuration saved to {ensemble_path}\")\n",
    "\n",
    "# Test the ensemble on some data\n",
    "sample_indices = np.random.choice(len(X_test), 5)\n",
    "for idx in sample_indices:\n",
    "    sample_X = X_test[idx:idx+1]\n",
    "    sample_y = y_test[idx]\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    lstm_pred = lstm_wrapper.predict(sample_X)[0]\n",
    "    gru_pred = gru_wrapper.predict(sample_X)[0]\n",
    "    rf_pred = rf_wrapper.predict(sample_X)[0]\n",
    "    gb_pred = gb_wrapper.predict(sample_X)[0]\n",
    "    \n",
    "    print(f\"\\nSample {idx}, True label: {sample_y}\")\n",
    "    print(f\"LSTM prediction: Class {lstm_pred[0]} with {lstm_pred[1]:.2f}% confidence\")\n",
    "    print(f\"GRU prediction: Class {gru_pred[0]} with {gru_pred[1]:.2f}% confidence\")\n",
    "    print(f\"RF prediction: Class {rf_pred[0]} with {rf_pred[1]:.2f}% confidence\")\n",
    "    print(f\"GB prediction: Class {gb_pred[0]} with {gb_pred[1]:.2f}% confidence\")\n",
    "    \n",
    "    # Simple majority vote\n",
    "    votes = [lstm_pred[0], gru_pred[0], rf_pred[0], gb_pred[0]]\n",
    "    vote_counts = np.bincount(votes, weights=[1.2, 1.1, 0.9, 1.0], minlength=3)\n",
    "    ensemble_class = np.argmax(vote_counts)\n",
    "    ensemble_confidence = (vote_counts[ensemble_class] / sum(vote_counts)) * 100\n",
    "    \n",
    "    print(f\"Ensemble prediction: Class {ensemble_class} with {ensemble_confidence:.2f}% confidence\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
